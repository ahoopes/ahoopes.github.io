<html lang="en"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Tutorial for learning the effect of registration hyperparameters with the HyperMorph strategy">
    <meta name="authors" content="Andrew Hoopes">
    <meta name="keywords" content="Image Registration, Deformation, Warp, Displacement, Deep Learning, VoxelMorph, Neuroimaging, Biomedical Imaging, MRI">
    <meta name="robots" content="all, index, follow">

    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QDT3X14ECB"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-QDT3X14ECB');
    </script>

    <title>HyperMorph Tutorial</title>

    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,400;0,600;0,700;1,400&amp;display=swap" rel="stylesheet">
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/fontawesome-all.min.css" rel="stylesheet">
    <link href="../css/styles.css" rel="stylesheet">
</head>

<body>
<main role="main" class="container">

<h1 class="mt-5" style="text-align: center;">Learning the Effect of Registration Hyperparameters</h1>
<p class="mt-4" style="text-align: center;">Andrew Hoopes and Adrian Dalca</p>

<p class="mt-5">Hyperparameter tuning is a time-consuming process in a wide variety of medical image analysis tasks, especially in the context of Deep Learning. In this tutorial we describe a strategy to alleviate challenges of hyperparameter tuning, and while we specifically focus on image registration, these concepts apply broadly.</p>
<p>Deformable registration is a common medical imaging technique that computes a dense transform (a deformation field) to accurately align two images. In more recent years, learning-based implementations of image registration have gained significant popularity.</p>
<p>However, hyperparameter tuning in learning-based registration involves training many separate models with various hyperparameter values, potentially leading to suboptimal results. <a href="https://ahoopes.github.io/hypermorph/">HyperMorph</a> addresses this inefficiency and removes the need to tune important registration hyperparameters during training by <em>learning</em> the effects of hyperparameters on deformation fields.</p>
<p>Generally, the HyperMorph strategy involves a secondary network, called a <em>hypernetwork</em>, that learns to predict the weights of a primary network for a given set of hyperparameter values. In effect, this strategy trains a single, rich model that enables rapid, fine-grained discovery of hyperparameter values from a continuous interval at <em>test-time</em>.</p>

<div class="container mt-4" style="width: 80%; min-width: 490px;">
  <div class="row">
    <div class="col my-auto"><img style="width: 100%;" src="resources/Moving.png" alt="Moving Image"></div>
    <div class="col my-auto"><img style="width: 100%;" src="resources/Tuner.gif" alt="HyperMorph Moved Image"></div>
    <div class="col my-auto"><img style="width: 100%;" src="resources/Fixed.png" alt="Fixed Image"></div>
  </div>
  <div style="text-align: center; margin-top: -1.6rem;" class="row">
    <div class="col">Moving Image</div>
    <div class="col">Moved Image</div>
    <div class="col">Fixed Image</div>
  </div>
</div>

<p class="mt-4">In this tutorial, we’ll develop and train a HyperMorph model to learn the effect of a common registration hyperparameter. This walk-through assumes some basic knowledge of learning-based image registration techniques, and some prior experience with the Keras and TensorFlow libraries might be useful. While the HyperMorph strategy can be implemented for any network, here we build off the open-source VoxelMorph registration library. For further information, there are plenty of useful resources, including papers and tutorials, on the <a href="https://github.com/voxelmorph/voxelmorph">VoxelMorph GitHub page</a>. The first section of this tutorial builds loosely off the main VoxelMorph tutorial.</p>

<h2 class="mt-5">Setup</h2>

<p>We’ll use TensorFlow 2.4 as well as the <a href="https://github.com/adalca/neurite">neurite</a> and <a href="https://github.com/voxelmorph/voxelmorph">voxelmorph</a> open-source python packages. The hypernetwork-based features are available in the development versions from GitHub.</p>

<pre><code class="language-bash">pip install git+https://github.com/adalca/pystrum.git@dev
pip install git+https://github.com/adalca/neurite.git@dev
pip install git+https://github.com/voxelmorph/voxelmorph.git@dev
</code></pre>

<p>Let’s import all of our modules, and make sure to disable TensorFlow’s eager execution, which is currently incompatible with the hypernetwork training framework.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> neurite <span class="hljs-keyword">as</span> ne
<span class="hljs-keyword">import</span> voxelmorph <span class="hljs-keyword">as</span> vxm
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

tf.compat.v1.disable_eager_execution()
</code></pre>

<h2 class="mt-5">Data</h2>

<p>Our goal is to align pairs of brain MR slices across a set of subjects gathered from the public <a href="https://www.oasis-brains.org/">OASIS</a> dataset. This sample subset consists of 414 2D images, which can be loaded directly from neurite.</p>
<pre><code class="language-python">images = ne.py.data.load_dataset(<span class="hljs-string">'2D-OASIS-TUTORIAL'</span>)
</code></pre>

<p>Each image is conformed to the shape (<em>height × width × channels</em>) where <em>channels</em> = 1, and the pixel-data is normalized between 0 and 1. We can visualize the first image in our dataset with the <code>ne.plot.slices()</code> function.</p>

<pre><code class="language-python">image = images[<span class="hljs-number">0</span>].squeeze()
image_shape = image.shape
ne.plot.slices(image)
</code></pre>
<p style="text-align: center;"><img src="resources/InputImage.png?" alt="" style="height:200px;"></p>
<p>We need to split the data evenly into training, validation, and test sets, each with 138 images.</p>
<pre><code class="language-python">train_data, validate_data, test_data = np.array_split(images, <span class="hljs-number">3</span>)
</code></pre>

<h2 class="mt-5">Training a registration network</h2>

<p>To start, let’s train a standard VoxelMorph registration network (with hard-coded hyperparameters) that learns to align our sample image data. We won’t cover the details of VoxelMorph in this tutorial, but include just enough information to enable us to illustrate the importance of the hypernet strategy. Detailed VoxelMorph-specific tutorials are available online.</p>
<p>We’ll use the <code>vxm.networks.VxmDense</code> class, which subclasses a Keras <code>Model</code> and requires some information about the input image shape.</p>
<pre><code class="language-python">model = vxm.networks.VxmDense(image_shape, int_steps=<span class="hljs-number">0</span>)
</code></pre>
<p>This VoxelMorph network is essentially a simple U-Net that takes as input an image pair (a moving and fixed image) and predicts a 2D deformation field that aligns the input moving image with the fixed target. By setting <code>int_steps</code> to 0, we’ve disabled diffeomorphic fields. By default, our configured model outputs two tensors: the first is the <em>moved</em> image (the <em>moving</em> image warped by the displacement field) and the second is the deformation field itself.</p>
<p>Now, let’s configure our loss functions, which are a focus of this tutorial. In registration, deformation fields are generally optimized with a multi-term loss that (1) maximizes the similarity between the moved and fixed images and (2) enforces some regularization on the field to prevent anatomically inaccurate displacements. In this tutorial, we’ll use MSE as an image similarity metric, and we’ll regularize (or smooth) the deformation field by minimizing the gradient around neighboring vectors.</p>
<p>The relative weight of these two loss terms <em>is balanced by an important hyperparameter λ</em>, which can vary (between 0 and 1 for purposes of this tutorial). The value chosen for λ can have substantial effects on the final registration accuracy (and has been a focus of ongoing research in the last few decades), but for now, we’ll use a hard-coded λ = 0.5.</p>
<pre><code class="language-python">lambda_weight = <span class="hljs-number">0.5</span>
image_loss = <span class="hljs-keyword">lambda</span> yt, yp: vxm.losses.MSE(<span class="hljs-number">0.1</span>).loss(yt, yp) * (<span class="hljs-number">1</span> - lambda_weight)
gradient_loss = <span class="hljs-keyword">lambda</span> yt, yp: vxm.losses.Grad(<span class="hljs-string">'l2'</span>).loss(yt, yp) * lambda_weight
losses = [image_loss, gradient_loss]
</code></pre>
<p>We need to configure a generator to produce data for each training step. At each iteration, we sample two random images, representing the input moving and fixed images, from the training set. In this tutorial, we’ll use a batch size of one and start with a standard VoxelMorph generator:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">voxelmorph_generator</span><span class="hljs-params">(data)</span>:</span>
    image_shape = data.shape[<span class="hljs-number">1</span>:]
    ndims = len(image_shape)
    <span class="hljs-comment"># the gradient loss is computed entirely on the predicted deformation,</span>
    <span class="hljs-comment"># so the reference data for the second model output can be ignored and</span>
    <span class="hljs-comment"># we can just provide an image of zeros</span>
    zeros = np.zeros([<span class="hljs-number">1</span>, *image_shape, ndims], dtype=<span class="hljs-string">'float32'</span>)
    <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:
        <span class="hljs-comment"># for each training iteration, grab a random pair of images to align</span>
        moving_image = data[np.random.randint(len(data))][np.newaxis]
        fixed_image = data[np.random.randint(len(data))][np.newaxis]
        inputs = [moving_image, fixed_image]
        outputs = [fixed_image, zeros]
        <span class="hljs-keyword">yield</span> (inputs, outputs)
</code></pre>
<p>Now that we’ve configured the losses and data generation, we can compile and train our model using the Adam optimizer. Let’s run the training for 500 epochs.</p>
<pre><code class="language-python">model.compile(optimizer=tf.keras.optimizers.Adam(lr=<span class="hljs-number">1e-4</span>), loss=losses)
gen = voxelmorph_generator(train_data)
history = model.fit_generator(gen, epochs=<span class="hljs-number">500</span>, steps_per_epoch=<span class="hljs-number">100</span>)
</code></pre>
<p>This step should take about 5 minutes on a GPU. While it’s unlikely that the model has properly converged at this point, it should be sufficient for <em>the sake of this tutorial</em>. Make sure to train your models fully (until the training or validation loss stops going down) under normal circumstances!</p>
<p>We can evaluate the model accuracy by predicting registrations across our validation images. Let’s grab an arbitrary moving and fixed image pair and visualize the resulting alignment.</p>
<pre><code class="language-python"><span class="hljs-comment"># predict the registration</span>
moving_image = validate_data[<span class="hljs-number">2</span>][np.newaxis]
fixed_image = validate_data[<span class="hljs-number">11</span>][np.newaxis]
moved_image, warp = model.predict([moving_image, fixed_image])

<span class="hljs-comment"># visualize the result</span>
slices = [moving_image, fixed_image, moved_image]
titles = [<span class="hljs-string">'Moving'</span>, <span class="hljs-string">'Fixed'</span>, <span class="hljs-string">'Moved'</span>]
ne.plot.slices(slices, titles=titles)
</code></pre>

<div class="text-center">
<img style="width: 80%;" src="resources/Registration.png" alt="HyperMorph">
</div>

<p>Not bad... but could the result be even better if we trained our model for a different value of λ? This is often a crucial decision when using a registration model.</p>
<p>The traditional way to answer this question is by training and evaluating a large series of models for different values of the hyperparameter. However, this can be incredibly tedious and time consuming, especially when working with large 3D networks that might take days or even weeks to converge!</p>

<h2 class="mt-5">Training a HyperMorph model</h2>

<p>To alleviate this burden, we can instead train a <em>single</em> HyperMorph model to <em>learn the effect</em> of the hyperparameters that we care about. This way, we can evaluate the impact of λ entirely at test time, without having to train multiple models. Let’s try it out!</p>
<p>Remember, HyperMorph is implemented by training a hypernetwork that predicts the weights of a target network. So, in our case, we want to configure a fully-connected network that takes a single λ value as input and outputs the weights of our primary VoxelMorph registration network. Let’s build a four-layer hypernetwork using a series of <code>Dense</code> Keras layers, and integrate it with our registration model via the <code>hyp_model</code> parameter.</p>

<pre><code class="language-python">hp_input = tf.keras.Input(shape=[<span class="hljs-number">1</span>])
x = tf.keras.layers.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>)(hp_input)
x = tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(x)
x = tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>)(x)
x = tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>)(x)
hypernetwork = tf.keras.Model(hp_input, x, name=<span class="hljs-string">'hypernetwork'</span>)

model = vxm.networks.VxmDense(image_shape, int_steps=<span class="hljs-number">0</span>, hyp_model=hypernetwork)
</code></pre>
<p>Important: in this new model, the only <em>trainable</em> weights are in the hypernetwork. For tutorial purposes, we’ve coded the dense hypernetwork from scratch, but it is also available directly via the <code>vxm.networks.HyperVxmDense</code> model class, which configures the hypernetwork architecture behind the scenes.</p>
<p>We’re almost ready to start training our HyperMorph model, but first we need to modify the losses and data generator a bit. During training, we want to make sure that the input λ value is properly used in the loss to facilitate appropriate gradients that are dependent on the hyperparameter. Let’s reconfigure our loss functions by setting <code>lambda_weight</code> equal to the hyperparameter input tensor defined above.</p>
<pre><code class="language-python">lambda_weight = hp_input
image_loss = <span class="hljs-keyword">lambda</span> yt, yp: vxm.losses.MSE(<span class="hljs-number">0.05</span>).loss(yt, yp) * (<span class="hljs-number">1</span> - lambda_weight)
gradient_loss = <span class="hljs-keyword">lambda</span> yt, yp: vxm.losses.Grad(<span class="hljs-string">'l2'</span>).loss(yt, yp) * lambda_weight
losses = [image_loss, gradient_loss]
</code></pre>
<p>Also, since our new model also accepts an input hyperparameter value, we’ll need to extend the generator to randomly sample a λ value between 0 and 1 for each training iteration. We can just build off of our existing generator function here.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hypermorph_generator</span><span class="hljs-params">(data)</span>:</span>
    <span class="hljs-comment"># initialize the base image pair generator</span>
    gen = voxelmorph_generator(data)
    <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:
        inputs, outputs = next(gen)
        <span class="hljs-comment"># append a random λ value to the input list</span>
        inputs = (*inputs, np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        <span class="hljs-keyword">yield</span> (inputs, outputs)
</code></pre>
<p>We’re ready to optimize the HyperMorph model. This time, we will train the network for a little longer.</p>
<pre><code class="language-python">model.compile(optimizer=tf.keras.optimizers.Adam(lr=<span class="hljs-number">1e-4</span>), loss=losses)
gen = hypermorph_generator(train_data)
history = model.fit_generator(gen, epochs=<span class="hljs-number">1000</span>, steps_per_epoch=<span class="hljs-number">100</span>)
</code></pre>
<p>Once trained, we now have a model that can predict corresponding deformation fields for any value of λ! Let’s visualize the predicted alignments for a few different regularization weights.</p>
<pre><code class="language-python">lambdas = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>).reshape((-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
slices = [model.predict([moving_image, fixed_image, hyp])[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> hyp <span class="hljs-keyword">in</span> lambdas]
titles = [<span class="hljs-string">'λ = %.2f'</span> % hyp <span class="hljs-keyword">for</span> hyp <span class="hljs-keyword">in</span> lambdas]
ne.plot.slices(slices, titles=titles)
</code></pre>
<img style="width: 100%;" src="resources/LambdaSweep.png" alt="">
<p>Here, we evaluated the model for a single image pair, but in practice, we can effectively choose ideal values for λ across a set of validation pairs, evaluating registrations either visually or via some registration metric, like the overlap of corresponding anatomical segmentations.</p>

<h2 class="mt-5">Takeaways</h2>

<p>The HyperMorph strategy facilitates fast hyperparameter search. <strong>Importantly</strong>, it also enables the ability to vary the hyperparameter at <em>test time</em>. This is critical since the often overlooked reality is that there’s no such thing as “one true optimal” hyperparameter value. For example, the optimal regularization weight can differ largely across dataset, modality, task, and even anatomical region, so it’s essential to have the ability to adapt hyperparameters on the fly.</p>
<p>Furthermore, while this tutorial focused on registration, the underlying strategy can be adapted to countless learning-based applications, far beyond the domain of registration or even medical imaging! If you’re interested in experimenting with hyper-architectures in TensorFlow, the <a href="https://github.com/adalca/neurite">neurite</a> library has Keras layers for easy hypernetwork integration. If you’d like a closer look at how HyperMorph is implemented, check out the VoxelMorph <a href="https://github.com/voxelmorph/voxelmorph/blob/dev/voxelmorph/tf/networks.py">source code</a>.</p>
<p>We hope this tutorial helps you try out hypernetworks for your own problem!</p>

</main>
<script src="../js/bootstrap.min.js"></script>
<script src="../js/scripts.js"></script>

</body>
</html>
